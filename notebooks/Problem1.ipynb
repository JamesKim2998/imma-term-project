{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "### 1-1. print title, author, submission date, abstract content, subjects using BeautifulSoup\n",
    "\n",
    "\n",
    "문제 1-1의 목표는 크롤링을 통해 페이지에 표시되는 문서를 긁어와 출력하는 코드를 작성하는 것입니다. 이를 위해 필요한 모듈은 `bs4` 패지키의 파싱을 위한 `BeautifulSoup` 모듈, 페이지 열람을 위한 `urllib` 패키지의 `urlopen` 모듈로, 코드 작성 첫 시작부분에 import를 통해 불러와 진행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 크롤링을 위해 문서가 담긴 페이지의 주소를 `url`변수에 저장한 후, `urlopen.read()` 함수를 통해 내용을 읽어와 `data`라는 변수에 저장하였습니다. 그 후 `BeautifulSoup` 함수를 통해 파싱을 진행하고 이를 `doc`이라는 변수에 저장하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://arxiv.org/abs/1811.06128'\n",
    "data = urlopen(url).read()\n",
    "doc = BeautifulSoup(data, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "페이지 소스보기를 통해 문서의 제목에 해당하는 부분의 html tag가 `h1`, class가 `title mathjax`임을 확인하였습니다. 이를 `doc.find.get_text()`함수를 통해 해당 제목만 추출하여 `title_orig`변수에 저장하였습니다. 페이지에 저장된 값이 제목뿐만이 아니라 \"Title: 제목\" 형식으로 되어 있어 ':'을 기준으로 한번만 스플릿한 뒤 그 뒤의 값만 가져와 출력하고 싶었기에 제목만 'title'이라는 변수에 저장하고 출력하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon\n"
     ]
    }
   ],
   "source": [
    "title_orig = doc.find(\"h1\", attrs={'class': \"title mathjax\"}).get_text()\n",
    "title = title_orig.split(':', maxsplit=1)[1]\n",
    "print(\"Title: \", title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 방법으로 저자에 해당하는 tag를 찾아 `find.get_text()` 함수를 통해 저장한 뒤 출력하였습니다. 저자 또한 \"Author: 저자\" 값으로 저장되어, ':'을 기준으로 한번만 스플릿한 뒤 그 뒤의 값만 가져오도록 하였습니다. `strip()` 함수를 통해 불필요한 공백도 제거하여 출력하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author:  Yoshua Bengio, Andrea Lodi, Antoine Prouvost\n"
     ]
    }
   ],
   "source": [
    "author_orig = doc.find(\"div\", attrs={'class': \"authors\"}).get_text()\n",
    "author = author_orig.split(':', maxsplit=1)[1]\n",
    "print(\"Author: \", author.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제출날짜도 같은 방법으로 불러왔고, 여기서 저장할 값은 날짜에 대한 정보만 있으면 되기 때문에, 불필요한 \"Submitted on\"이라는 글자를 제거하기 위해, 공백을 기준으로 두번 스플릿하여 맨 뒤의 날짜만 가져와 값을 저장하였습니다. 또한 불필요한 기호 \")\"를  `replace()` 함수를 통해 제거하여 출력하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  15 Nov 2018\n"
     ]
    }
   ],
   "source": [
    "date_orig = doc.find(\"div\", attrs={'class': \"dateline\"}).get_text()\n",
    "date_mod = date_orig.split(' ', maxsplit=2)[2]\n",
    "date = date_mod.replace(\")\", \"\")\n",
    "print(\"Date: \", date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초록 내용, 서브젝트도 동일하게 `split()`함수와 `strip()`함수를 통해 불필요한 내용과, 공백을 제거하여 출력하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:  This paper surveys the recent attempts, both from the machine learning and\n",
      "operations research communities, at leveraging machine learning to solve\n",
      "combinatorial optimization problems. Given the hard nature of these problems,\n",
      "state-of-the-art methodologies involve algorithmic decisions that either\n",
      "require too much computing time or are not mathematically well defined. Thus,\n",
      "machine learning looks like a promising candidate to effectively deal with\n",
      "those decisions. We advocate for pushing further the integration of machine\n",
      "learning and combinatorial optimization and detail methodology to do so. A main\n",
      "point of the paper is seeing generic optimization problems as data points and\n",
      "inquiring what is the relevant distribution of problems to use for learning on\n",
      "a given task.\n",
      "Subjects:  Machine Learning (cs.LG); Machine Learning (stat.ML)\n"
     ]
    }
   ],
   "source": [
    "# TODO - print abstract\n",
    "abstracts = doc.find(\"blockquote\", attrs={'class': \"abstract mathjax\"}).get_text()\n",
    "abstract = abstracts.split(': ', maxsplit=1)[1]\n",
    "print(\"Abstract: \", abstract.strip())\n",
    "\n",
    "# TODO - print subjects\n",
    "subjects = doc.find(\"td\", attrs={'class': \"tablecell subjects\"}).get_text()\n",
    "print(\"Subjects: \", subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Tokenize abstract content by words and POS-Tag tokenized words\n",
    "\n",
    "문제 1-2의 목표는 저장된 텍스트 문서를 토큰화 하고, Pos tagging하는 것입니다. 이를 위해 `nltk`패키지의 `word_tokenize()` 함수와 `pos_tag()`함수를 import하여 진행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 1-1에서 저장하였던 초록을 `word_tokenize()` 함수를 통해  토큰화하였고, 이를 `tokenized_words` 변수에 저장하여 출력하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize Result:  ['This', 'paper', 'surveys', 'the', 'recent', 'attempts', ',', 'both', 'from', 'the', 'machine', 'learning', 'and', 'operations', 'research', 'communities', ',', 'at', 'leveraging', 'machine', 'learning', 'to', 'solve', 'combinatorial', 'optimization', 'problems.', 'Given', 'the', 'hard', 'nature', 'of', 'these', 'problems', ',', 'state-of-the-art', 'methodologies', 'involve', 'algorithmic', 'decisions', 'that', 'either', 'require', 'too', 'much', 'computing', 'time', 'or', 'are', 'not', 'mathematically', 'well', 'defined.', 'Thus', ',', 'machine', 'learning', 'looks', 'like', 'a', 'promising', 'candidate', 'to', 'effectively', 'deal', 'with', 'those', 'decisions.', 'We', 'advocate', 'for', 'pushing', 'further', 'the', 'integration', 'of', 'machine', 'learning', 'and', 'combinatorial', 'optimization', 'and', 'detail', 'methodology', 'to', 'do', 'so.', 'A', 'main', 'point', 'of', 'the', 'paper', 'is', 'seeing', 'generic', 'optimization', 'problems', 'as', 'data', 'points', 'and', 'inquiring', 'what', 'is', 'the', 'relevant', 'distribution', 'of', 'problems', 'to', 'use', 'for', 'learning', 'on', 'a', 'given', 'task', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_words = word_tokenize(abstract, language=None, preserve_line=True)\n",
    "print(\"Tokenize Result: \", tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화된 단어들을 pos tagging하기 위해 `pos_tag()` 함수를 사용하였고, 이를 `tagged_list` 변수에 저장하여 출력하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos_tag Result:  [('This', 'DT'), ('paper', 'NN'), ('surveys', 'VBZ'), ('the', 'DT'), ('recent', 'JJ'), ('attempts', 'NNS'), (',', ','), ('both', 'DT'), ('from', 'IN'), ('the', 'DT'), ('machine', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('operations', 'NNS'), ('research', 'NN'), ('communities', 'NNS'), (',', ','), ('at', 'IN'), ('leveraging', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('to', 'TO'), ('solve', 'VB'), ('combinatorial', 'JJ'), ('optimization', 'NN'), ('problems.', 'NN'), ('Given', 'NNP'), ('the', 'DT'), ('hard', 'JJ'), ('nature', 'NN'), ('of', 'IN'), ('these', 'DT'), ('problems', 'NNS'), (',', ','), ('state-of-the-art', 'JJ'), ('methodologies', 'NNS'), ('involve', 'VBP'), ('algorithmic', 'JJ'), ('decisions', 'NNS'), ('that', 'IN'), ('either', 'DT'), ('require', 'VB'), ('too', 'RB'), ('much', 'JJ'), ('computing', 'VBG'), ('time', 'NN'), ('or', 'CC'), ('are', 'VBP'), ('not', 'RB'), ('mathematically', 'RB'), ('well', 'RB'), ('defined.', 'JJ'), ('Thus', 'NNP'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('looks', 'NNS'), ('like', 'IN'), ('a', 'DT'), ('promising', 'JJ'), ('candidate', 'NN'), ('to', 'TO'), ('effectively', 'RB'), ('deal', 'VB'), ('with', 'IN'), ('those', 'DT'), ('decisions.', 'NNS'), ('We', 'PRP'), ('advocate', 'VBP'), ('for', 'IN'), ('pushing', 'VBG'), ('further', 'PDT'), ('the', 'DT'), ('integration', 'NN'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('combinatorial', 'JJ'), ('optimization', 'NN'), ('and', 'CC'), ('detail', 'NN'), ('methodology', 'NN'), ('to', 'TO'), ('do', 'VB'), ('so.', 'VB'), ('A', 'NNP'), ('main', 'JJ'), ('point', 'NN'), ('of', 'IN'), ('the', 'DT'), ('paper', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('generic', 'JJ'), ('optimization', 'NN'), ('problems', 'NNS'), ('as', 'IN'), ('data', 'NNS'), ('points', 'NNS'), ('and', 'CC'), ('inquiring', 'VBG'), ('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('relevant', 'JJ'), ('distribution', 'NN'), ('of', 'IN'), ('problems', 'NNS'), ('to', 'TO'), ('use', 'VB'), ('for', 'IN'), ('learning', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('given', 'VBN'), ('task', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_list = pos_tag(tokenized_words, tagset=None, lang='eng')\n",
    "print(\"Pos_tag Result: \", tagged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. sort tokenized words by frequency\n",
    "\n",
    "문제 1-3의 목표는 토큰화된 단어들을 출현 빈도가 높은 순으로 정렬하는 것입니다.\n",
    "\n",
    "각 단어들의 출현 빈도를 저장하기 위해 `token_count`라는 딕셔너리를 생성하였습니다. 그 후 for문을 통해 앞서 저장된 리스트에서 각 단어를 보고 딕셔너리의 key 중에 존재하는 단어이면 그 key에 해당하는 값을 1 증가시키고, 없는 단어이면 그 단어를 새로운 key값으로 추가한 뒤 해당 값을 1로 두는 방법으로 딕셔너리 내에 각 단어들에 대한 빈도를 `token_count` 변수에 저장하였습니다. 이후 `sorted` 함수를 사용하여 `token_count`값을 비교 기준으로 내림차순 정렬했습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Token by frequency:  [('the', 6), ('learning', 5), ('and', 4), ('to', 4), ('of', 4), ('machine', 4), (',', 4), ('problems', 3), ('optimization', 3), ('a', 2), ('combinatorial', 2), ('paper', 2), ('for', 2), ('is', 2), ('that', 1), ('on', 1), ('communities', 1), ('methodology', 1), ('decisions', 1), ('require', 1), ('relevant', 1), ('so.', 1), ('detail', 1), ('too', 1), ('promising', 1), ('We', 1), ('Given', 1), ('use', 1), ('task', 1), ('defined.', 1), ('generic', 1), ('with', 1), ('.', 1), ('algorithmic', 1), ('hard', 1), ('deal', 1), ('state-of-the-art', 1), ('nature', 1), ('advocate', 1), ('further', 1), ('what', 1), ('leveraging', 1), ('recent', 1), ('attempts', 1), ('decisions.', 1), ('these', 1), ('involve', 1), ('well', 1), ('data', 1), ('methodologies', 1), ('A', 1), ('or', 1), ('at', 1), ('mathematically', 1), ('are', 1), ('do', 1), ('inquiring', 1), ('those', 1), ('Thus', 1), ('seeing', 1), ('surveys', 1), ('time', 1), ('distribution', 1), ('as', 1), ('operations', 1), ('point', 1), ('effectively', 1), ('points', 1), ('pushing', 1), ('from', 1), ('looks', 1), ('main', 1), ('This', 1), ('like', 1), ('given', 1), ('integration', 1), ('solve', 1), ('both', 1), ('research', 1), ('either', 1), ('problems.', 1), ('computing', 1), ('much', 1), ('not', 1), ('candidate', 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique_word_list, word_count_list = np.unique(tokenized_words, return_counts=True)\n",
    "token_count = dict(zip(unique_word_list, word_count_list))\n",
    "\n",
    "token_sorted = {}\n",
    "token_sorted = sorted([(n,m) for n,m in token_count.items()], key=lambda token_count: -token_count[1])\n",
    "print(\"Sorting Token by frequency: \", token_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. plot WordCloud and apply stopwords to WordCloud\n",
    "\n",
    "\n",
    "문제 1-4의 목표는 1-3의 토큰화되고 정렬된 단어들을 이용하여 word cloud를 만들고, 만들어진 word cloud보다 abstract의 내용을 더 잘 표현하는 word cloud를 생성하는 것입니다.\n",
    "\n",
    "1-3의 token_count dictionary를 이용하여 word cloud를 만들었을 때, 결과는 다음과 같습니다. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# using 'token_count' dictionary to plot wordcloud\n",
    "wordcloud = WordCloud().generate_from_frequencies(token_count)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 word cloud는 'to, of, and' 등의 stop words들이 다수 포함되어 있고 '(apostrophe)와 .(period)등의 특수글자들이 등장하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved word cloud \n",
    "# replace special characters \n",
    "import re\n",
    "letters_only = re.sub('[^a-zA-Z]', ' ', abstract)\n",
    "\n",
    "# convert each letter into lowercase and tokenize\n",
    "lowered_letters = letters_only.lower()\n",
    "lowered_letters = word_tokenize(lowered_letters, language=None, preserve_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특수문자들을 제거하기 위해 정규식표현식 re 모듈의 `.sub()`함수를 이용하여 abstract안의 영어문자가 아닌 이외의 것들을 공백으로 바꿔주었습니다. \n",
    "그 후, `.lower()` 함수를 이용하여 대문자를 소문자로 변환한 후 토큰화했습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "stopping_words = []\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for token in lowered_letters:\n",
    "    if token not in stops:\n",
    "        stopping_words.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "new_token_count = {}\n",
    "for token in stopping_words:\n",
    "    if token in new_token_count:\n",
    "        new_token_count[token] = new_token_count[token] + 1\n",
    "    else:\n",
    "        new_token_count[token] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표준 Stopwords 목록에 포함된 단어는 제외하고, stopwords가 아닌 단어들을 대상으로 lemmatizing하여 `stopping_words`라는 array에 저장했습니다.\n",
    "\n",
    "Porter Stemmer등의 Stemmer을 이용하지 않은 이유는, 단어가 아닌 stem(어간)을 출력하여 사전에 없는 결과가 나올수 있기 때문입니다. Lemmatizing은 Stemming보다 더 많은 정보(문맥, 형태소, 사전적 분석)를 고려하여 더 높은 단어정확도를 보입니다.\n",
    "\n",
    "위에서 모든 처리를 마친 `new_token_count`를 이용하여 새로운 word cloud인 `new_worldcloud`를 생성한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wordcloud = WordCloud().generate_from_frequencies(new_token_count)\n",
    "plt.imshow(new_wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
